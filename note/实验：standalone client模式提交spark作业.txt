local模式下，我们都不会放到生产机器上面去提交
local模式，其实仅仅用于eclipse中运行spark作业，以及打断点，调试spark作业来用
通常，用local模式执行，我们都会手工生成一份数据，来使用

通常情况下来说，部署在测试机器上去，进行测试运行spark作业的时候
都是使用client模式（standalone client模式，或standalone cluster模式）
client模式下，提交作业以后，driver在本机启动，可以实时看到详细的日志信息，方便你追踪和排查错误

standalone模式下提交spark作业

standalone模式提交，唯一与local区别，就是一定要想办法将master设置成spark://master_ip:port，比如spark://192.168.0.103:7077

三种设置master的方式
1、硬编码: SparkConf.setMaster("spark://IP:PORT")
2、spark-submit: --master spark://IP:PORT
3、spark-shell: --master spark://IP:PORT

通常来说，上面三种写法，使用第二种，是最合适

一般我们都是用spark-submit脚本来提交spark application的，很少用spark-shell，spark-shell仅仅用于实验和测试一些特别简单的spark作业
使用spark-submit脚本来提交时，还可以指定两种deploy mode，spark standalone cluster支持client mode和cluster mode
client mode下，你在哪台机器上用spark-submit脚本提交application，driver就会在那台机器上启动
cluster mode下，driver会通过master进程，随机被分配给某一个worker进程来启动

这里我们就知道了，standalone模式，是要在spark-submit中，用--master指定Master进程的URL
其次，使用standalone client模式或cluster模式，是要在spark-submit中，使用--deploy-mode client/cluster来设置
默认，如果你不设置--deploy-mode，那么就是client模式

使用spark-submit脚本来提交application时，application jar是会自动被分发到所有worker节点上去的
对于你的application依赖的额外jar包，可以通过spark-submit脚本中的--jars标识，来指定，可以使用逗号分隔多个jar
比如说，你写spark-sql的时候，有的时候，在作业中，要往mysql中写数据，此时可能会出现找不到mysql驱动jar包的情况
此时，就需要你手动在spark-submit脚本中，使用--jars，加入一些依赖的jar包

我们提交standalone client模式的作业
1、--master和--deploy-mode，来提交作业
2、观察作业执行日志，可以看到去连接spark://192.168.0.103:7077 URL的master
3、观察一些spark web ui，可以看到completed applications一栏中，有我们刚刚运行的作业，比对一下ui上的applicationId和日志中的applicationId
4、重新再执行一次作业，一执行，立即看一下jps查看进程，看看standalone client模式提交作业的时候，当前机器上会启动哪些进程
	SparkSubmit: 也就是我们的driver进程，在本机上启动（spark-submit所在的机器）
	CoarseGrainedExecutorBackend（内部持有一个Executor对象）: 在你的执行spark作业的worker机器上，肯定会给你的作业分配和启动一个executor进程，具体名字就是那个
